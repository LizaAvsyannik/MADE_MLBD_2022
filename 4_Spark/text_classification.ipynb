{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work was conducted with [Spark 3.2.0](//spark.apache.org/releases/spark-release-3-2-0.html) and [Zeppelin 0.10.1](https://hub.docker.com/layers/zeppelin/apache/zeppelin/0.10.1/images/sha256-9c1b5ddd6225ad45cedc327a853f6f13e45797ff419260d20bb9c14f5cbe3a87?context=explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "val train_raw: DataFrame = spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"escape\", \"\\\\\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(\"/notebook/data/train.csv\")\n",
    "    .limit(20000)\n",
    "\n",
    "train_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "// remove NaN and drop \"id\" column as we won't need it anymore\n",
    "val train: DataFrame = train_raw.na.drop()\n",
    "    .drop(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view some toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "train.filter(train(\"toxic\") === 1)\n",
    "    .select(\"comment_text\")\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "\n",
    "val tokenizer: Tokenizer = new Tokenizer()\n",
    "    .setInputCol(\"comment_text\")\n",
    "    .setOutputCol(\"comment_text_tokenized\")\n",
    "                    \n",
    "val train_tokenized: DataFrame = tokenizer.transform(train)\n",
    "    .drop(\"comment_text\")\n",
    "\n",
    "train_tokenized.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Extraction: HashingTF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "import org.apache.spark.ml.feature.HashingTF\n",
    "\n",
    "val hashingTF: HashingTF = new HashingTF()\n",
    "    .setInputCol(\"comment_text_tokenized\")\n",
    "    .setOutputCol(\"raw_features\")\n",
    "    .setNumFeatures(scala.math.pow(2, 10).toInt)\n",
    "\n",
    "val train_tf: DataFrame = hashingTF.transform(train_tokenized)\n",
    "    .drop(\"comment_text_tokenized\")\n",
    "\n",
    "train_tf.select(\"raw_features\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "import org.apache.spark.ml.feature.IDF\n",
    "import org.apache.spark.ml.feature.IDFModel\n",
    "\n",
    "val idf: IDF = new IDF()\n",
    "    .setInputCol(\"raw_features\")\n",
    "    .setOutputCol(\"features\")\n",
    "val idf_model: IDFModel = idf.fit(train_tf)\n",
    "\n",
    "val train_tfidf: DataFrame = idf_model.transform(train_tf)\n",
    "    .drop(\"raw_features\")\n",
    "\n",
    "train_tfidf.select(\"features\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Exctraction: Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "import org.apache.spark.ml.feature.Word2Vec\n",
    "import org.apache.spark.ml.feature.Word2VecModel\n",
    "\n",
    "val word2vec: Word2Vec = new Word2Vec()\n",
    "    .setInputCol(\"comment_text_tokenized\")\n",
    "    .setOutputCol(\"features\")\n",
    "val word2vec_model: Word2VecModel = word2vec.fit(train_tokenized)\n",
    "\n",
    "val train_w2v: DataFrame = word2vec_model.transform(train_tokenized)\n",
    "    .drop(\"comment_text_tokenized\")\n",
    "\n",
    "train_w2v.select(\"features\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.classification.LogisticRegressionModel\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "def train_loop(train_df: DataFrame, test_df: DataFrame): Unit = {\n",
    "    val targets: List[String] = List(\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\")\n",
    "\n",
    "    for (target <- targets) {\n",
    "        val lr: LogisticRegression = new LogisticRegression()\n",
    "            .setMaxIter(500)\n",
    "            .setRegParam(0.01)\n",
    "            .setElasticNetParam(0.08)\n",
    "            .setFeaturesCol(\"features\")\n",
    "            .setLabelCol(target)\n",
    "        \n",
    "        val model_lr: LogisticRegressionModel = lr.fit(train_df\n",
    "            .withColumn(target, col(target).cast(\"int\")))\n",
    "        \n",
    "        val test_df_filtered: DataFrame = test_df\n",
    "            .withColumn(target, col(target).cast(\"int\"))\n",
    "            .filter(test_df(target) !== -1)\n",
    "        val test_predictions: DataFrame = model_lr.transform(test_df_filtered)\n",
    "\n",
    "        val evaluator: BinaryClassificationEvaluator = new BinaryClassificationEvaluator()\n",
    "            .setLabelCol(target)\n",
    "            .setRawPredictionCol(\"probability\")\n",
    "            .setMetricName(\"areaUnderROC\")\n",
    "        \n",
    "        val roc_auc: Double = evaluator.evaluate(test_predictions)\n",
    "        \n",
    "        println(s\"Target: $target\")\n",
    "        println(s\"ROC-AUC: $roc_auc\")   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Read Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "// read comments\n",
    "val test_raw: DataFrame = spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"escape\", \"\\\\\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(\"/notebook/data/test.csv\")\n",
    "\n",
    "test_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "// read labels\n",
    "val test_labels: DataFrame = spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"escape\", \"\\\\\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(\"/notebook/data/test_labels.csv\")\n",
    "\n",
    "test_labels.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "// join comments and labels, drop NaN and id\n",
    "val test: DataFrame = test_raw\n",
    "    .join(test_labels, test_raw(\"id\") === test_labels(\"id\"), \"inner\")\n",
    "    .na.drop()\n",
    "    .drop(\"id\")\n",
    "\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "// tokenize\n",
    "val test_tokenized: DataFrame = tokenizer.transform(test)\n",
    "    .drop(\"comment_text\")\n",
    "\n",
    "test_tokenized.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Experiments: HashingTF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "val pows: List[Int] = List(9, 10, 11, 12, 13)\n",
    "\n",
    "println(\"HashingTF-IDF\")\n",
    "println(\"------------------------\")\n",
    "for (pow <- pows) {\n",
    "    val num_features: Int = scala.math.pow(2, pow).toInt\n",
    "    println(s\"Num Features: $num_features\")\n",
    "    \n",
    "    hashingTF.setNumFeatures(num_features)\n",
    "    val train_tf: DataFrame = hashingTF.transform(train_tokenized)\n",
    "        .drop(\"comment_text_tokenized\")\n",
    "\n",
    "    val idf_model: IDFModel = idf.fit(train_tf)\n",
    "    val train_tfidf = idf_model.transform(train_tf)\n",
    "        .drop(\"raw_features\")\n",
    "    \n",
    "    val test_tf: DataFrame = hashingTF.transform(test_tokenized)\n",
    "        .drop(\"comment_text_tokenized\")\n",
    "    val test_tfidf: DataFrame = idf_model.transform(test_tf)\n",
    "        .drop(\"raw_features\")\n",
    "\n",
    "    train_loop(train_tfidf, test_tfidf)\n",
    "    println(\"------------------------\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conclusion***: inscreasing numFeatures parameter has positive impact on metrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Experiments: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "val test_w2v: DataFrame = word2vec_model.transform(train_tokenized)\n",
    "    .drop(\"comment_text_tokenized\")\n",
    "\n",
    "println(\"Word2Vec\")\n",
    "println(\"------------------------\")\n",
    "train_loop(train_w2v, test_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conslusion:*** Word2Vec showed better results than HashingTF-IDF, which is not suprising, as in general TF-IDF performs worse than Word2Vec, so its approximate version would be too.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "name": "text_classification"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
