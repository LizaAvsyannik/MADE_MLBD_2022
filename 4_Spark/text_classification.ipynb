{
  "metadata": {
    "name": "text_classification",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Work was conducted with [Spark 3.2.0](//spark.apache.org/releases/spark-release-3-2-0.html) and [Zeppelin 0.10.1](https://hub.docker.com/layers/zeppelin/apache/zeppelin/0.10.1/images/sha256-9c1b5ddd6225ad45cedc327a853f6f13e45797ff419260d20bb9c14f5cbe3a87?context\u003dexplore)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1. Read Train Data"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql.DataFrame\n\nval train_raw: DataFrame \u003d spark.read\n    .option(\"multiline\", \"true\")\n    .option(\"quote\", \"\\\"\")\n    .option(\"header\", \"true\")\n    .option(\"escape\", \"\\\\\")\n    .option(\"escape\", \"\\\"\")\n    .csv(\"/notebook/data/train.csv\")\n    .limit(20000)\n\ntrain_raw.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// remove NaN and drop \"id\" column as we won\u0027t need it anymore\nval train: DataFrame \u003d train_raw.na.drop()\n    .drop(\"id\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Let\u0027s view some toxic comments"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ntrain.filter(train(\"toxic\") \u003d\u003d\u003d 1)\n    .select(\"comment_text\")\n    .take(2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2. Tokenize data\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.ml.feature.Tokenizer\n\nval tokenizer: Tokenizer \u003d new Tokenizer()\n    .setInputCol(\"comment_text\")\n    .setOutputCol(\"comment_text_tokenized\")\n                    \nval train_tokenized: DataFrame \u003d tokenizer.transform(train)\n    .drop(\"comment_text\")\n\ntrain_tokenized.show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3. Feature Extraction: HashingTF-IDF\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.ml.feature.HashingTF\n\nval hashingTF: HashingTF \u003d new HashingTF()\n    .setInputCol(\"comment_text_tokenized\")\n    .setOutputCol(\"raw_features\")\n    .setNumFeatures(scala.math.pow(2, 10).toInt)\n\nval train_tf: DataFrame \u003d hashingTF.transform(train_tokenized)\n    .drop(\"comment_text_tokenized\")\n\ntrain_tf.select(\"raw_features\").take(2)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.ml.feature.IDF\nimport org.apache.spark.ml.feature.IDFModel\n\nval idf: IDF \u003d new IDF()\n    .setInputCol(\"raw_features\")\n    .setOutputCol(\"features\")\nval idf_model: IDFModel \u003d idf.fit(train_tf)\n\nval train_tfidf: DataFrame \u003d idf_model.transform(train_tf)\n    .drop(\"raw_features\")\n\ntrain_tfidf.select(\"features\").take(2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 4. Feature Exctraction: Word2Vec\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.ml.feature.Word2Vec\nimport org.apache.spark.ml.feature.Word2VecModel\n\nval word2vec: Word2Vec \u003d new Word2Vec()\n    .setInputCol(\"comment_text_tokenized\")\n    .setOutputCol(\"features\")\nval word2vec_model: Word2VecModel \u003d word2vec.fit(train_tokenized)\n\nval train_w2v: DataFrame \u003d word2vec_model.transform(train_tokenized)\n    .drop(\"comment_text_tokenized\")\n\ntrain_w2v.select(\"features\").take(2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5. Train Loop\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.classification.LogisticRegressionModel\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n\ndef train_loop(train_df: DataFrame, test_df: DataFrame): Unit \u003d {\n    val targets: List[String] \u003d List(\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\")\n\n    for (target \u003c- targets) {\n        val lr: LogisticRegression \u003d new LogisticRegression()\n            .setMaxIter(500)\n            .setRegParam(0.01)\n            .setElasticNetParam(0.08)\n            .setFeaturesCol(\"features\")\n            .setLabelCol(target)\n        \n        val model_lr: LogisticRegressionModel \u003d lr.fit(train_df\n            .withColumn(target, col(target).cast(\"int\")))\n        \n        val test_df_filtered: DataFrame \u003d test_df\n            .withColumn(target, col(target).cast(\"int\"))\n            .filter(test_df(target) !\u003d\u003d -1)\n        val test_predictions: DataFrame \u003d model_lr.transform(test_df_filtered)\n\n        val evaluator: BinaryClassificationEvaluator \u003d new BinaryClassificationEvaluator()\n            .setLabelCol(target)\n            .setRawPredictionCol(\"probability\")\n            .setMetricName(\"areaUnderROC\")\n        \n        val roc_auc: Double \u003d evaluator.evaluate(test_predictions)\n        \n        println(s\"Target: $target\")\n        println(s\"ROC-AUC: $roc_auc\")   \n    }\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 6. Read Test Data\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// read comments\nval test_raw: DataFrame \u003d spark.read\n    .option(\"multiline\", \"true\")\n    .option(\"quote\", \"\\\"\")\n    .option(\"header\", \"true\")\n    .option(\"escape\", \"\\\\\")\n    .option(\"escape\", \"\\\"\")\n    .csv(\"/notebook/data/test.csv\")\n\ntest_raw.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// read labels\nval test_labels: DataFrame \u003d spark.read\n    .option(\"multiline\", \"true\")\n    .option(\"quote\", \"\\\"\")\n    .option(\"header\", \"true\")\n    .option(\"escape\", \"\\\\\")\n    .option(\"escape\", \"\\\"\")\n    .csv(\"/notebook/data/test_labels.csv\")\n\ntest_labels.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// join comments and labels, drop NaN and id\nval test: DataFrame \u003d test_raw\n    .join(test_labels, test_raw(\"id\") \u003d\u003d\u003d test_labels(\"id\"), \"inner\")\n    .na.drop()\n    .drop(\"id\")\n\ntest.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// tokenize\nval test_tokenized: DataFrame \u003d tokenizer.transform(test)\n    .drop(\"comment_text\")\n\ntest_tokenized.show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 7. Experiments: HashingTF-IDF"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval pows: List[Int] \u003d List(9, 10, 11, 12, 13)\n\nprintln(\"HashingTF-IDF\")\nprintln(\"------------------------\")\nfor (pow \u003c- pows) {\n    val num_features: Int \u003d scala.math.pow(2, pow).toInt\n    println(s\"Num Features: $num_features\")\n    \n    hashingTF.setNumFeatures(num_features)\n    val train_tf: DataFrame \u003d hashingTF.transform(train_tokenized)\n        .drop(\"comment_text_tokenized\")\n\n    val idf_model: IDFModel \u003d idf.fit(train_tf)\n    val train_tfidf \u003d idf_model.transform(train_tf)\n        .drop(\"raw_features\")\n    \n    val test_tf: DataFrame \u003d hashingTF.transform(test_tokenized)\n        .drop(\"comment_text_tokenized\")\n    val test_tfidf: DataFrame \u003d idf_model.transform(test_tf)\n        .drop(\"raw_features\")\n\n    train_loop(train_tfidf, test_tfidf)\n    println(\"------------------------\")\n}\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "***Conclusion***: inscreasing numFeatures parameter has positive impact on metrics. \n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 8. Experiments: Word2Vec"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval test_w2v: DataFrame \u003d word2vec_model.transform(train_tokenized)\n    .drop(\"comment_text_tokenized\")\n\nprintln(\"Word2Vec\")\nprintln(\"------------------------\")\ntrain_loop(train_w2v, test_w2v)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "***Conslusion:*** Word2Vec showed better results than HashingTF-IDF, which is not suprising, as in general TF-IDF performs worse than Word2Vec, so its approximate version would be too.\n"
    }
  ]
}